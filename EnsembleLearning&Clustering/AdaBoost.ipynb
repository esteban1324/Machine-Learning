{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random dataset with 20 samples. Each sample should have two input\n",
    "features and 1 output label (-1 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67795555  0.5129588   1.        ]\n",
      " [ 0.62370571  0.47714247  1.        ]\n",
      " [ 0.46025981  0.95086487 -1.        ]\n",
      " [ 0.9845756   0.85442255  1.        ]\n",
      " [ 0.59109035  0.20319637 -1.        ]\n",
      " [ 0.26236016  0.06641525 -1.        ]\n",
      " [ 0.64693501  0.71873295  1.        ]\n",
      " [ 0.35104926  0.95254348 -1.        ]\n",
      " [ 0.59547743  0.99450621 -1.        ]\n",
      " [ 0.54367354  0.07171263 -1.        ]\n",
      " [ 0.14068668  0.20727832 -1.        ]\n",
      " [ 0.50552741  0.53539293  1.        ]\n",
      " [ 0.53145508  0.23291118 -1.        ]\n",
      " [ 0.60550138  0.70289106 -1.        ]\n",
      " [ 0.06662988  0.2366768   1.        ]\n",
      " [ 0.15895731  0.97330243 -1.        ]\n",
      " [ 0.71025347  0.08285516  1.        ]\n",
      " [ 0.54836989  0.1424483  -1.        ]\n",
      " [ 0.10721273  0.61757862 -1.        ]\n",
      " [ 0.91187275  0.90530837 -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(120)\n",
    "\n",
    "# generate random data with last column as target\n",
    "data = np.random.rand(20,2)\n",
    "target = np.random.choice([-1,1], size=(20,))\n",
    "\n",
    "data_with_y = np.concatenate((data, target.reshape(-1,1)), axis=1)\n",
    "\n",
    "print(data_with_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a weak learer decision tree that only has a max-depth of 1. Output the feature and threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Threshold: 0.6146035194396973\n"
     ]
    }
   ],
   "source": [
    "# minimal stump\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# train it \n",
    "\n",
    "clf.fit(data, target)\n",
    "\n",
    "# then extract the feature and threshold of the it's split\n",
    "feature_index = clf.tree_.feature[0]\n",
    "threshold_index = clf.tree_.threshold[0]\n",
    "\n",
    "print(f\"Feature: {feature_index}, Threshold: {threshold_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the prediction errors, calculate the coefficient Î±j and updated weights. Out-\n",
    "put the updated weights.  (Follow AdaBoost Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0210042  0.0210042  0.0210042  0.0210042  0.0210042  0.0210042\n",
      " 0.0210042  0.0210042  0.0210042  0.0210042  0.0210042  0.11902381\n",
      " 0.0210042  0.0210042  0.11902381 0.0210042  0.0210042  0.0210042\n",
      " 0.0210042  0.11902381]\n",
      "Weights: [0.01631471 0.01631471 0.01631471 0.01631471 0.05302281 0.05302281\n",
      " 0.01631471 0.01631471 0.01631471 0.05302281 0.05302281 0.09245003\n",
      " 0.05302281 0.05302281 0.09245003 0.01631471 0.01631471 0.05302281\n",
      " 0.05302281 0.09245003]\n",
      "Weights: [0.03785581 0.03785581 0.00976924 0.03785581 0.03175003 0.03175003\n",
      " 0.03785581 0.00976924 0.00976924 0.03175003 0.03175003 0.21451624\n",
      " 0.03175003 0.03175003 0.05535903 0.00976924 0.03785581 0.03175003\n",
      " 0.03175003 0.05535903]\n",
      "Weights: [0.02651761 0.02651761 0.02138339 0.02651761 0.02224057 0.02224057\n",
      " 0.02651761 0.02138339 0.02138339 0.02224057 0.02224057 0.15026644\n",
      " 0.02224057 0.06949601 0.03877844 0.02138339 0.08286063 0.02224057\n",
      " 0.06949601 0.12117253]\n",
      "Weights: [0.01631414 0.01631414 0.01315547 0.05867104 0.04920797 0.04920797\n",
      " 0.05867104 0.01315547 0.01315547 0.04920797 0.04920797 0.09244679\n",
      " 0.04920797 0.04275528 0.02385724 0.01315547 0.05097745 0.04920797\n",
      " 0.04275528 0.07454767]\n",
      "Weights: [0.01084443 0.01084443 0.00874478 0.03900015 0.0327098  0.0327098\n",
      " 0.03900015 0.00874478 0.00874478 0.0327098  0.0327098  0.20424538\n",
      " 0.0327098  0.02842054 0.05270849 0.00874478 0.03388602 0.0327098\n",
      " 0.02842054 0.16470032]\n",
      "Weights: [0.008421   0.008421   0.00679056 0.03028469 0.05923089 0.05923089\n",
      " 0.03028469 0.00679056 0.00679056 0.05923089 0.05923089 0.15860215\n",
      " 0.05923089 0.05146389 0.04092959 0.00679056 0.02631343 0.05923089\n",
      " 0.05146389 0.12789433]\n",
      "Weights: [0.00616008 0.00616008 0.01104837 0.0221537  0.04332827 0.04332827\n",
      " 0.0221537  0.01104837 0.01104837 0.04332827 0.04332827 0.11601982\n",
      " 0.04332827 0.08373267 0.0299406  0.01104837 0.04281242 0.04332827\n",
      " 0.08373267 0.20808637]\n",
      "Weights: [0.01108196 0.01108196 0.0071759  0.03985439 0.02814167 0.02814167\n",
      " 0.03985439 0.0071759  0.0071759  0.02814167 0.02814167 0.20871905\n",
      " 0.02814167 0.05438428 0.05386298 0.0071759  0.07701933 0.02814167\n",
      " 0.05438428 0.13515188]\n",
      "Weights: [0.00849982 0.00849982 0.00550389 0.03056816 0.04706373 0.04706373\n",
      " 0.03056816 0.00550389 0.00550389 0.04706373 0.04706373 0.16008669\n",
      " 0.04706373 0.09095151 0.0413127  0.00550389 0.05907352 0.04706373\n",
      " 0.09095151 0.10366096]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
       "       -1.,  1., -1.,  1., -1., -1., -1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m is num_itr \n",
    "def adaBoost_train(data, m):\n",
    "    # initialize the weights and set them to uniform weights that sum to 1\n",
    "    n = data.shape[0]\n",
    "    weights = np.ones(n) / n\n",
    "\n",
    "    alphas = []\n",
    "    errors = []\n",
    "    y_preds = np.zeros(n)\n",
    "    \n",
    "    \n",
    "    for _ in range(m):\n",
    "       \n",
    "        # initialize a decision tree with weak stump and train the tree using the data, target and weights  \n",
    "        classifier = DecisionTreeClassifier(max_depth=1)\n",
    "        classifier.fit(data[:, :-1], data[:, -1], sample_weight=weights)\n",
    "        \n",
    "        # predict class labels \n",
    "        y_pred = classifier.predict(data[:, :-1])\n",
    "                \n",
    "        error = np.sum(weights * (y_pred != data[:, -1])) / np.sum(weights)\n",
    "        \n",
    "        errors.append(error)\n",
    "        \n",
    "        # compute the alpha and append to list\n",
    "        alpha = 0.5 * np.log((1 - error)/ float(error) )\n",
    "        alphas.append(alpha)\n",
    "        \n",
    "        y_preds += alpha * y_pred\n",
    "        \n",
    "        # update weights here \n",
    "        weights = weights * np.exp(-alpha * data[:, -1] * y_pred)\n",
    "        # output updated weights\n",
    "        print(f\"Weights: {weights}\")\n",
    "        \n",
    "        \n",
    "        # normalize the weights so they sum to 1 \n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        print(f\"Iteration: {_}, feature:{classifier.tree_.feature[0]}, threshold: {classifier.tree_.threshold[0]} , Alpha: {alpha}\")\n",
    "        \n",
    "    # compute final prediction here \n",
    "    final_pred = np.sign(y_preds)\n",
    "    \n",
    "    return final_pred \n",
    "    \n",
    "adaBoost_train(data_with_y, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
