{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random dataset with 20 samples. Each sample should have two input\n",
    "features and 1 output label (-1 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67795555  0.5129588   1.        ]\n",
      " [ 0.62370571  0.47714247  1.        ]\n",
      " [ 0.46025981  0.95086487 -1.        ]\n",
      " [ 0.9845756   0.85442255  1.        ]\n",
      " [ 0.59109035  0.20319637 -1.        ]\n",
      " [ 0.26236016  0.06641525 -1.        ]\n",
      " [ 0.64693501  0.71873295  1.        ]\n",
      " [ 0.35104926  0.95254348 -1.        ]\n",
      " [ 0.59547743  0.99450621 -1.        ]\n",
      " [ 0.54367354  0.07171263 -1.        ]\n",
      " [ 0.14068668  0.20727832 -1.        ]\n",
      " [ 0.50552741  0.53539293  1.        ]\n",
      " [ 0.53145508  0.23291118 -1.        ]\n",
      " [ 0.60550138  0.70289106 -1.        ]\n",
      " [ 0.06662988  0.2366768   1.        ]\n",
      " [ 0.15895731  0.97330243 -1.        ]\n",
      " [ 0.71025347  0.08285516  1.        ]\n",
      " [ 0.54836989  0.1424483  -1.        ]\n",
      " [ 0.10721273  0.61757862 -1.        ]\n",
      " [ 0.91187275  0.90530837 -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(120)\n",
    "\n",
    "# generate random data with last column as target\n",
    "data = np.random.rand(20,2)\n",
    "target = np.random.choice([-1,1], size=(20,))\n",
    "\n",
    "data_with_y = np.concatenate((data, target.reshape(-1,1)), axis=1)\n",
    "\n",
    "print(data_with_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train a weak learer decision tree that only has a max-depth of 1. Output the feature and threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Threshold: 0.6146035194396973\n"
     ]
    }
   ],
   "source": [
    "# minimal stump\n",
    "clf = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# train it \n",
    "\n",
    "clf.fit(data, target)\n",
    "\n",
    "# then extract the feature and threshold of the it's split\n",
    "feature_index = clf.tree_.feature[0]\n",
    "threshold_index = clf.tree_.threshold[0]\n",
    "\n",
    "print(f\"Feature: {feature_index}, Threshold: {threshold_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the prediction errors, calculate the coefficient Î±j and updated weights. Out-\n",
    "put the updated weights.  (Follow AdaBoost Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Error: 0.15000000000000002, Alpha: 0.8673005276940532, Weights: [0.02941176 0.02941176 0.02941176 0.02941176 0.02941176 0.02941176\n",
      " 0.02941176 0.02941176 0.02941176 0.02941176 0.02941176 0.16666667\n",
      " 0.02941176 0.02941176 0.16666667 0.02941176 0.02941176 0.02941176\n",
      " 0.02941176 0.16666667]\n",
      "Iteration: 1, Error: 0.23529411764705885, Alpha: 0.589327498170823, Weights: [0.01923077 0.01923077 0.01923077 0.01923077 0.0625     0.0625\n",
      " 0.01923077 0.01923077 0.01923077 0.0625     0.0625     0.10897436\n",
      " 0.0625     0.0625     0.10897436 0.01923077 0.01923077 0.0625\n",
      " 0.0625     0.10897436]\n",
      "Iteration: 2, Error: 0.20512820512820512, Alpha: 0.6772728314026553, Weights: [0.046875   0.046875   0.01209677 0.046875   0.03931452 0.03931452\n",
      " 0.046875   0.01209677 0.01209677 0.03931452 0.03931452 0.265625\n",
      " 0.03931452 0.03931452 0.06854839 0.01209677 0.046875   0.03931452\n",
      " 0.03931452 0.06854839]\n",
      "Iteration: 3, Error: 0.2424395161290322, Alpha: 0.5696755598236067, Weights: [0.03093812 0.03093812 0.02494802 0.03093812 0.0259481  0.0259481\n",
      " 0.03093812 0.02494802 0.02494802 0.0259481  0.0259481  0.17531603\n",
      " 0.0259481  0.08108108 0.04524285 0.02494802 0.0966736  0.0259481\n",
      " 0.08108108 0.14137214]\n",
      "Iteration: 4, Error: 0.217564870259481, Alpha: 0.6399569787806595, Weights: [0.01977041 0.01977041 0.01594255 0.07110092 0.05963303 0.05963303\n",
      " 0.07110092 0.01594255 0.01594255 0.05963303 0.05963303 0.11203231\n",
      " 0.05963303 0.05181329 0.02891156 0.01594255 0.06177739 0.05963303\n",
      " 0.05181329 0.09034113]\n",
      "Iteration: 5, Error: 0.23128500360643217, Alpha: 0.600534776526518, Weights: [0.01285939 0.01285939 0.01036961 0.0462466  0.03878748 0.03878748\n",
      " 0.0462466  0.01036961 0.01036961 0.03878748 0.03878748 0.24219537\n",
      " 0.03878748 0.03370124 0.06250203 0.01036961 0.04018224 0.03878748\n",
      " 0.03370124 0.1953026 ]\n",
      "Iteration: 6, Error: 0.3001273248680169, Alpha: 0.4233458124932495, Weights: [0.00918695 0.00918695 0.00740821 0.0330393  0.06461837 0.06461837\n",
      " 0.0330393  0.00740821 0.00740821 0.06461837 0.06461837 0.17302816\n",
      " 0.06461837 0.0561449  0.04465243 0.00740821 0.02870682 0.06461837\n",
      " 0.0561449  0.13952724]\n",
      "Iteration: 7, Error: 0.3101567133422468, Alpha: 0.39969337717781206, Weights: [0.00665872 0.00665872 0.01194269 0.02394696 0.04683554 0.04683554\n",
      " 0.02394696 0.01194269 0.01194269 0.04683554 0.04683554 0.12541121\n",
      " 0.04683554 0.09051053 0.03236418 0.01194269 0.04627794 0.04683554\n",
      " 0.09051053 0.22493022]\n",
      "Iteration: 8, Error: 0.2652646909788824, Alpha: 0.5093910746028744, Weights: [0.01255109 0.01255109 0.00812721 0.04513786 0.03187239 0.03187239\n",
      " 0.04513786 0.00812721 0.00812721 0.03187239 0.03187239 0.23638881\n",
      " 0.03187239 0.06159397 0.06100356 0.00812721 0.08722973 0.03187239\n",
      " 0.06159397 0.15306888]\n",
      "Iteration: 9, Error: 0.3144222878484524, Alpha: 0.38976245503952733, Weights: [0.00915366 0.00915366 0.00592727 0.03291958 0.05068405 0.05068405\n",
      " 0.03291958 0.00592727 0.00592727 0.05068405 0.05068405 0.17240118\n",
      " 0.05068405 0.09794785 0.04449063 0.00592727 0.06361768 0.05068405\n",
      " 0.09794785 0.11163496]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
       "       -1.,  1., -1.,  1., -1., -1., -1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m is num_itr \n",
    "def adaBoost_train(data,m):\n",
    "    # initialize the weights and set them to uniform weights that sum to 1\n",
    "    n = data.shape[0]\n",
    "    weights = np.ones(n) / n\n",
    "\n",
    "    alphas = []\n",
    "    errors = []\n",
    "    y_preds = np.zeros(n)\n",
    "    \n",
    "    \n",
    "    for _ in range(m):\n",
    "       \n",
    "        # initialize a decision tree with weak stump and train the tree using the data, target and weights  \n",
    "        classifier = DecisionTreeClassifier(max_depth=1)\n",
    "        classifier.fit(data[:, :-1], data[:, -1], sample_weight=weights)\n",
    "        \n",
    "        # predict class labels \n",
    "        y_pred = classifier.predict(data[:, :-1])\n",
    "        \n",
    "        \n",
    "        error = np.sum(weights * (y_pred != data[:, -1]))\n",
    "        \n",
    "        errors.append(error)\n",
    "        \n",
    "        # compute the alpha and append to list\n",
    "        alpha = 0.5 * np.log((1 - error)/ float(error) )\n",
    "        alphas.append(alpha)\n",
    "        \n",
    "        y_preds += alpha * y_pred\n",
    "        \n",
    "        # update weights here \n",
    "        weights = weights * np.exp(-alpha * data[:, -1] * y_pred)\n",
    "        \n",
    "        \n",
    "        # normalize the weights so they sum to 1 \n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        print(f\"Iteration: {_}, Error: {error}, Alpha: {alpha}, Weights: {weights}\")\n",
    "        \n",
    "    # compute final prediction here \n",
    "    final_pred = np.sign(y_preds)\n",
    "    \n",
    "    return final_pred \n",
    "    \n",
    "adaBoost_train(data_with_y, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
